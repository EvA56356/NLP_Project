Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
Loading features from cached file datasets\cached_-train_lstm_128_jd
Loading features from cached file datasets\cached_-eval_lstm_512_jd
Loading features from cached file datasets\cached_-test_lstm_512_jd
Training/evaluation parameters Namespace(adam_epsilon=1e-08, adv_epsilon=1.0, adv_name='word_embeddings', cache_dir='', config_name='', data_dir='datasets', data_format='json', device=device(type='cpu'), do_adv=False, do_eval=False, do_lower_case=False, do_test=False, do_train=True, eval_all_checkpoints=False, eval_max_seq_length=512, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, id2label={0: 'bad review', 1: 'good review'}, label2id={'bad review': 0, 'good review': 1}, label_list=['bad review', 'good review'], learning_rate=0.0002, local_rank=-1, logging_steps=100, loss_type='ce', max_grad_norm=1.0, max_steps=-1, model_name_or_path=None, model_type='lstm', n_gpu=0, n_jobs=4, no_cuda=False, num_train_epochs=100, output_dir='outputs/lstm', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=64, per_gpu_train_batch_size=32, predict_checkpoints=0, pretrained_weight=None, save_steps=50, seed=42, server_ip='', server_port='', task_name='jd', tokenizer_name='', train_max_seq_length=128, vocab_file=None, warmup_proportion=0.1, weight_decay=0.01, word_type=True)
***** Running training *****
  Num examples = 1600
  Num Epochs = 100
  Instantaneous batch size per GPU = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 5000


